12.0版本就是我们的powerhash了。通过9.0-11.2版本的参数设置的种种改进，于是提出根据power-law分布中的pareto principle设置big group和small group的比例，
这个比例只作为参考值，即big group和small group的比例是0.2， 人为设定比例建议设定在0.2左右，例如0.1, 0.3。并且认识的small group的总大小只有20%左右，更加
贴近实际情况。另外，在11.2中引入的accumulate_num扩大因子beta一般设置为2。
这样整个powerhash的实际参数就只有大小组比例alpha和accumulate_num扩大因子beta。

整体的算法思想仍然沿用8.0版本：
   1.遍历原始文件，通过count-min sketch粗略统计每个group的 rough size,同时生成记录<key, valuesize>的中间文件key_file。
     然后通过big group和small group的比例alpha获取阈值：对count-min sketch的某一行进行降序排序， 从排好序的这一列中的第（count_min_length*alpha）个
     值就是阈值。
   2.遍历key_file, 通过key_file中的key从count-min sketch中获取rough size， 然后通过阈值判断是否是big group，如果是，在建立索引，累加valuesize和
     keysize获取精确 group size；如果是small group，则直接通过hash分区的方式累加到对应的accumulate partition，获取每个parititon的size。然后，
     通过big group size和partition size转化得到对应的offset index。
   3.遍历原始文件，通过<key，value>中的key查找 offset index, 如果有对应key， 则属于big group，将其根据offset写入对应位置。如果没有key,
     则属于small group, 将其通过hash方式写入对应的partition. 遍历完成之后，big group以完成groupBy, small group仍然无序，这个时候再将
     partition逐个读入内存，利用hash groupBy方式进行group, 完成之后将结果输出到结果文件。


回顾一下算法中每一次进步，如何最终得到powerhash:
   通过1.0版本认识了bHash,也就是indexing-filling方法。
   通过2.0版本引入count-min sketch对group size进行粗略统计
   通过3.0版本引入big group和small group的单独处理的思想，但是只是处理big group，small group不作处理
   通过4.0版本引入key_file的二进制块读出和写入的工程思想。
   通过5.0版本引入：还是通过精确group size生成输出索引更合适的想法，key_file记录<key, valuesize>。
                  同时，引入将small group统一写入accumulate单元，但是仍然不作处理。
   通过6.0版本引入：将accumulate单元分区的思想，也就是对small groups进行分区的思想，但是small group仍然不作处理。
   通过7.3版本引入适用于所有数据集的key_file二进制读写方法，即key结构体设置为<char[], int>类型。
   通过8.0版本正是确定算法的思想：大组和小组分开处理的核心思想。
通过9.0-11.2版本进行参数设置改进：
    通过9.0版本的参数设置，虽然方法有漏洞，但是可以确定index offset大小的边界值，即确定最多能够容纳多少个big group的输出索引。
    通过11.0版本，将一直沿用的将原始文件进行spill分区的小技巧,减少了参数的设置，而且计算出的big group num可以直接作为参数，不需要再认为设定。
    通过11.1版本，将big groups和small group分开放，免得还要记录每个small group partition的offset.
    通过11.2版本，提出人为设置big group和small group的比例，同时发现pareto principle.
最终获得12.0版本，即powerhash:
    powerhash思想和8.0一样，不过调参方式是参照pareto principle定律。